{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Edited Resume Parser",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO3RtuyGSvprn3GEAWkmM4v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saigirishgilly98/Major_Project/blob/main/Edited_Resume_Parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwpZhWt7OFI6",
        "outputId": "c953edf2-52a6-4e93-a737-53c5d3d364ae"
      },
      "source": [
        "! pip install pdfminer.six\r\n",
        "import io"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.6/dist-packages (20201018)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.6/dist-packages (from pdfminer.six) (3.3.1)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.6/dist-packages (from pdfminer.six) (2.3.0)\n",
            "Requirement already satisfied: chardet; python_version > \"3.0\" in /usr/local/lib/python3.6/dist-packages (from pdfminer.six) (3.0.4)\n",
            "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from cryptography->pdfminer.six) (1.15.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.6/dist-packages (from cryptography->pdfminer.six) (1.14.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.12->cryptography->pdfminer.six) (2.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrVD7uQNu_ct"
      },
      "source": [
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.pdfinterp import PDFPageInterpreter\n",
        "from pdfminer.pdfinterp import PDFResourceManager\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as fh:\n",
        "        # iterate over all pages of PDF document\n",
        "        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
        "            # creating a resoure manager\n",
        "            resource_manager = PDFResourceManager()\n",
        "            \n",
        "            # create a file handle\n",
        "            fake_file_handle = io.StringIO()\n",
        "            \n",
        "            # creating a text converter object\n",
        "            converter = TextConverter(\n",
        "                                resource_manager, \n",
        "                                fake_file_handle, \n",
        "                                codec='utf-8', \n",
        "                                laparams=LAParams()\n",
        "                        )\n",
        "\n",
        "            # creating a page interpreter\n",
        "            page_interpreter = PDFPageInterpreter(\n",
        "                                resource_manager, \n",
        "                                converter\n",
        "                            )\n",
        "\n",
        "            # process current page\n",
        "            page_interpreter.process_page(page)\n",
        "            \n",
        "            # extract text\n",
        "            text = fake_file_handle.getvalue()\n",
        "            yield text\n",
        "\n",
        "            # close open handles\n",
        "            converter.close()\n",
        "            fake_file_handle.close()\n",
        "\n",
        "# calling above function and extracting text\n",
        "text = ''\n",
        "for page in extract_text_from_pdf('sandhya_resume.pdf'):\n",
        "    text += ' ' + page\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "YyES1UxmOqqs",
        "outputId": "1690725b-7f02-4b63-a257-cba92a04ad11"
      },
      "source": [
        "text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" . \\n\\nSANDHYA CHANDRAMOHAN \\nLos Angeles, California 92614 | (949) 522-0190 | sandhya2392@gmail.com| https://github.com/s4ndhyac \\n\\nEDUCATION \\n \\nUNIVERSITY OF CALIFORNIA, IRVINE  Master of Computer Science \\nINDIAN INSTITUTE OF TECHNOLOGY \\n\\nBachelor of Technology \\n\\n  \\n\\nEXPERIENCE \\n \\nDEVOPS ENGINEERING INTERN, Tiger Connect, Santa Monica, CA \\n\\nCGPA: 3.60/4.00* \\nCGPA: 8.07/10.00 \\n\\nExpected: Dec 2019 \\nGraduated: July 2014 \\n\\nJune 2019 - Aug 2019 \\n\\n●  Enabled real-time SLO, SLI metrics by streaming logs from the Nginx load balancer through Logstash to Kafka Topics and \\n\\ningesting them into Hadoop data warehouses for persistent storage and analytics \\n\\n●  Reduced image cache size by 80% by enabling dynamic optimization of images on a CDN by creating an AWS Lambda @ Edge \\n\\nserverless function and aggregating logs via Kafka streams to the Elasticsearch server \\n\\n●  Formulated a migration plan and executed a zero downtime DNS provider migration writing Ansible roles \\n●  Accomplished 100% storage-balanced Kafka partitions fetching Kafka broker and partition metrics, writing it to Zookeeper \\n\\nnodes and using DataDog's kafka-kit tool \\n\\nSENIOR MEMBER OF TECHNICAL STAFF, Capital Float, Mumbai, India \\n\\nMay 2017 - Aug 2018 \\n\\n●  Lead Developer of Amazon Pay EMI, analogous to the Amazon.com store card in the US used by millions of users daily \\n●  Pioneered the Publish-Subscribe Design Pattern to scale the platform, enabling it to support 10 times the existing users \\nImplemented Distributed Locking using Redis NoSQL database to handle 100% of all race conditions in a multi-user web app \\n● \\n●  Reduced deployment downtime by 100% adopting the Blue-Green deployment strategy, setting up a Continuous Integration / \\n\\nContinuous Delivery process in a distributed environment using Jenkins and AWS CodeDeploy \\n\\nMEMBER OF TECHNICAL STAFF, Capital Float, Mumbai, India \\n\\n May 2016 - May 2017 \\n\\nImproved database performance by 30% and ensured 99.99% availability by setting up MySQL Master-Slave Replication \\n\\n●  Spearheaded a distributed design by extracting database operations into Stored Procedures with Row-level locking \\n● \\n●  Created a large-scale data pipeline enabling an ETL process for data transfers to a Datawarehouse \\n●  Led to 100% real-time processing of all payments by engineering a thread-safe application, synchronizing critical operations \\n\\nSOFTWARE DEVELOPER, Capital Float, Mumbai, India \\n\\n \\n\\nMay 2015 - May 2016 \\n\\n●  Redesigned and refactored a legacy WPF application in the Singleton pattern to a multi-tier RESTful Web Application in the \\nMVC design pattern following Object Oriented Design principles using ASP.NET Web API 2 and the WCF Framework \\n\\n●  Spearheaded the Unit of Work design pattern to carry out a business operation as one atomic transaction \\n●  Ensured fast AGILE development cycles and code robustness by integrating the NUnit Unit Testing Framework \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nPROJECTS \\n\\nBasicOS ● Contributed to the Open Source Unix - like OS xv6’s kernel ● Repository \\n● \\n\\nImplemented Unix system calls to get the memory dump, support multi-threading, mutex locking, conditional variables, \\nsemaphores and linked list file-addressing to theoretically support files of infinite size \\n●  Developed a unix shell with IO redirection, pipes and several simple unix programs like ls, cp \\n\\nApr 2019 – Jun 2019 \\n\\n \\n\\n \\n\\nRDBMS ● Multi-layer relational database with optimized index and query systems ● Repository \\n●  Devised  several  join  algorithms  such  as  the  Block  Nested  Loop  Join,  the  Index  Nested  Loop  Join  and  Sort  Merge  Join \\n\\nOct 2018 – Dec 2018 \\n\\nalgorithms using the External sorting algorithm for sorting \\n\\n●  Built an Indexing Component via a B+ Tree supporting range predicates, managing persistent indexes over structured data \\n\\nExploits and Attacks ● SEEDLabs Security Education Labs on Ubuntu 16.04 VM ● Repository \\n●  Attacked the environment variable  and Set-UID programs, exploited the  buffer overflow vulnerability and performed packet \\n\\nJan 2019 – Mar 2019 \\n\\n \\n\\nsniffing and snooping and SQL injection attacks \\n\\nAlgorithms & Data Structures ● Hashing Algorithms | Tree Data Structures ● Repository ● Repository  Jan 2019 – Mar 2019 \\n●  Formulated optimized implementations in Java and evaluated the performance of several hashing algorithms - Linear probing, \\nDouble hashing, Chained hashing, Cuckoo hashing and Tree Data Structures - Binary Search Tree, AVL Tree, Treap, Splay tree  \\nImplemented the methods in the IHashingAlgorithm and ITree interface respectively for each  \\n\\n● \\n\\nSKILLS \\n\\n●  Languages: (Over 5000 lines) Java, C#, Python, MySQL, Ansible, Chef, Bash | (Over 1000 lines) C++, C, Ruby \\n●  Frameworks/ Libraries: (Extensive) ASP.NET, Web2Py | (Comfortable) Flask, J2EE, Jetty, Jersey, Dropwizard \\n●  Other Tools/ Technologies: (Extensive) AWS, Docker, Git, Redis, Kafka, Elasticsearch, Logstash \\n●  Relevant Courses: Algorithms, Data Structures, Data Management, Operating Systems, Computer Security \\n\\n\\x0c\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAlgGGW7O1sK",
        "outputId": "c855bb95-3b66-4537-adf5-b91eb2bbbe37"
      },
      "source": [
        "! pip install spacy\r\n",
        "! python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (51.3.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (51.3.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvSV4W44wRll"
      },
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# load pre-trained model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# initialize matcher with a vocab\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "def extract_name(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "    \n",
        "    # First name and Last name are always Proper Nouns\n",
        "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
        "    \n",
        "    matcher.add('NAME', None, pattern)\n",
        "    \n",
        "    matches = matcher(nlp_text)\n",
        "    \n",
        "    for match_id, start, end in matches:\n",
        "        span = nlp_text[start:end]\n",
        "        print(span.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR8wAQS4O5oL",
        "outputId": "0801d3fb-6dff-49da-d421-e5d7ee3fb1b6"
      },
      "source": [
        "extract_name(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Los Angeles\n",
            "| sandhya2392@gmail.com|\n",
            "Computer Science\n",
            "INDIAN INSTITUTE\n",
            "Tiger Connect\n",
            "Santa Monica\n",
            "Kafka Topics\n",
            "AWS Lambda\n",
            "SENIOR MEMBER\n",
            "TECHNICAL STAFF\n",
            "Capital Float\n",
            "Amazon Pay\n",
            "Pay EMI\n",
            "Design Pattern\n",
            "Redis NoSQL\n",
            "AWS CodeDeploy\n",
            "Capital Float\n",
            "MySQL Master\n",
            "Slave Replication\n",
            "Stored Procedures\n",
            "Capital Float\n",
            "Object Oriented\n",
            "Oriented Design\n",
            "ASP.NET Web\n",
            "Web API\n",
            "WCF Framework\n",
            "NUnit Unit\n",
            "Unit Testing\n",
            "Testing Framework\n",
            "Open Source\n",
            "Source Unix\n",
            "OS xv6\n",
            "Indexing Component\n",
            "SEEDLabs Security\n",
            "Security Education\n",
            "Education Labs\n",
            "● Repository\n",
            "Data Structures\n",
            "Algorithms |\n",
            "| Tree\n",
            "Tree Data\n",
            "Data Structures\n",
            "Binary Search\n",
            "Search Tree\n",
            "C#\n",
            "Bash |\n",
            "Web2Py |\n",
            "Tools/ Technologies\n",
            "Data Structures\n",
            "Data Management\n",
            "Operating Systems\n",
            "Computer Security\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au6JOCWaPT-V"
      },
      "source": [
        "# val0 = input(\"Is the name correct? (Y/N)\")\r\n",
        "# if val0=='N'\r\n",
        "# :\r\n",
        "#   name = input(\"Enter corrent name\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1eFcv7lwoI0"
      },
      "source": [
        "import re\n",
        "\n",
        "def extract_mobile_number(text):\n",
        "    flag = 0\n",
        "    phone = re.findall(re.compile(r'(\\d{3}-\\d{3}-\\d{4})'), text)\n",
        "    if len(phone) == 0:\n",
        "        phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\n",
        "        flag = 1\n",
        "    print(phone)\n",
        "    if phone:\n",
        "        if flag == 1:\n",
        "            number = ''.join(phone[0])\n",
        "        else:\n",
        "            number = ''.join(phone[0].split('-'))\n",
        "        if len(number) > 10:\n",
        "            return '+' + number\n",
        "        else:\n",
        "            return number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8zw9WF-O9vi",
        "outputId": "789158df-99c7-4c1b-99f7-8651e5d0192f"
      },
      "source": [
        "mobile_number = extract_mobile_number(text)\r\n",
        "print(mobile_number)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('', '949', '', '522', '0190', '')]\n",
            "9495220190\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WfmE2K2QOaO",
        "outputId": "211f629c-5d82-45e6-979a-bcc7d8d51172"
      },
      "source": [
        "val1 = input(\"Is the phone number correct? (Y/N)\\n\")\r\n",
        "if val1=='N':\r\n",
        "  mobile_number = input(\"Enter corrent phone number\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is the phone number correct? (Y/N)\n",
            "Y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3w3WQi_xOyo"
      },
      "source": [
        "import re\n",
        "\n",
        "def extract_email(email):\n",
        "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", email)\n",
        "    if email:\n",
        "        try:\n",
        "            return email[0].split()[0].strip(';')\n",
        "        except IndexError:\n",
        "            return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_Suwf__PBDS",
        "outputId": "efffda73-5c9d-4122-9540-c9ff49954aaf"
      },
      "source": [
        "email = extract_email(text)\r\n",
        "print(email)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sandhya2392@gmail.com|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fLQHhuNQeOq",
        "outputId": "757e51f8-6d30-41f4-ce36-2e2e6f034d4d"
      },
      "source": [
        "val2 = input(\"Is the email correct? (Y/N)\\n\")\r\n",
        "if val2=='N':\r\n",
        "  email = input(\"Enter corrent email\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is the email correct? (Y/N)\n",
            "Y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "QMCvZjMYPDCT",
        "outputId": "4fd221e7-e3f7-411f-f8bb-bd08f1db91f2"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "skills_dataset = pd.read_csv('linkedin_skill.txt', sep='\\n', header=None)\r\n",
        "skills_dataset.columns = ['skill']\r\n",
        "skills_dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>skill</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(ISC)2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>.NET</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>.NET CLR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>.NET Compact Framework</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>.NET Framework</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    skill\n",
              "0                  (ISC)2\n",
              "1                    .NET\n",
              "2                .NET CLR\n",
              "3  .NET Compact Framework\n",
              "4          .NET Framework"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fnm2IS2PG2z",
        "outputId": "252645ff-6426-41c7-f79c-74387eedf6c5"
      },
      "source": [
        "skills = []\r\n",
        "for i in range(len(skills_dataset)):\r\n",
        "    skills.append(str(skills_dataset.loc[i, 'skill']).lower())\r\n",
        "skills[-20:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['zoominfo',\n",
              " 'zoomtext',\n",
              " 'zoomerang',\n",
              " 'zope',\n",
              " 'zotero',\n",
              " 'zsh',\n",
              " 'zuken',\n",
              " 'zultys',\n",
              " 'zulu education products',\n",
              " 'zuludesk',\n",
              " 'zumba',\n",
              " 'zumba instruction',\n",
              " 'zuora',\n",
              " 'zymography',\n",
              " 'zynx',\n",
              " 'zyxel',\n",
              " 'z/os',\n",
              " 'z/vm',\n",
              " 'zlinux',\n",
              " 'zseries']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 269
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OeajW-iyWOT"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# load pre-trained model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def extract_skills(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "\n",
        "    # removing stop words and implementing word tokenization\n",
        "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
        "    \n",
        "    skillset = []\n",
        "    \n",
        "    # check for one-grams (example: python)\n",
        "    for token in tokens:\n",
        "        if token.lower() in skills:\n",
        "            skillset.append(token)\n",
        "    \n",
        "    # check for bi-grams and tri-grams (example: machine learning)\n",
        "    for token in nlp_text.noun_chunks:\n",
        "        token = token.text.lower().strip()\n",
        "        if token in skills:\n",
        "            skillset.append(token)\n",
        "    \n",
        "    return [i.capitalize() for i in set([i.lower() for i in skillset])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t922sd0ePKFy",
        "outputId": "d409451f-0516-4fe6-b036-d29e0e00504d"
      },
      "source": [
        "skills = extract_skills(text)\r\n",
        "print(skills)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Python', 'Range', 'Jetty', 'Io', 'Cp', 'Hadoop', 'Load', 'Pipes', 'Pattern', 'Testing', 'Structured data', 'Computer security', 'Ansible', 'Design', 'Ruby', 'Devops', 'Singleton', 'Partition', 'C#', 'Los angeles', 'Web2py', 'Computer science', 'Operations', 'Elasticsearch', 'Mumbai', 'Row', 'Asp.net web api', 'Jenkins', 'Availability', 'Kafka', 'Unix', 'Nunit', 'Replication', 'Ubuntu', 'Rdbms', 'Asp.net', 'Technology', 'Mysql', 'Binary', 'Sli', 'Java', 'Principles', 'Mvc', 'Security', 'Logstash', 'Bash', 'Web', 'Application', 'Languages', 'Stored procedures', 'Nosql', 'Search', 'C', 'Sql', 'Framework', 'Database', 'Led', 'Wpf', 'Vulnerability', 'Capital', 'Merge', 'App', 'Strategy', 'Redis', 'Optimization', 'Aws', 'Docker', 'Algorithms', 'Payments', 'C++', 'Git', 'Sorting', 'It', 'Metrics', 'Agile', 'Api', 'Kernel', 'Flask', 'Processing', 'Vm', 'Streams', 'Etl', 'Operating systems', 'Edge', 'Nginx', 'Writing', 'Connect', 'Pay', 'Mar', 'Indexing', 'Pipeline', 'Analytics', 'Dropwizard', 'Amazon', 'Subscribe', 'Repository', 'Storage', 'Store', 'Avl']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuQ7aEskSILS",
        "outputId": "40d910d2-4a45-48e3-c2d3-0f1f0baa9ba9"
      },
      "source": [
        "# skills_np = np.array(skills)\r\n",
        "# skills_dict = dict(enumerate(skills_np.flatten(), 1)) \r\n",
        "# print(skills_dict)\r\n",
        "# arr=[]\r\n",
        "# print(\"Choose the relevant skills, enter Stop when done\")\r\n",
        "\r\n",
        "\r\n",
        "# x=1\r\n",
        "# # numpy.append(array, value, axis)\r\n",
        "# while(x == 1):\r\n",
        "#     n = input()\r\n",
        "#     if n =='Stop':\r\n",
        "#       x=0\r\n",
        "#       break\r\n",
        "#     else:\r\n",
        "#       arr.append(n)\r\n",
        "\r\n",
        "skills_np = np.array(skills)\r\n",
        "skills_dict = dict(enumerate(skills_np.flatten(), 1)) \r\n",
        "print(skills_dict)\r\n",
        "arr=[]\r\n",
        "print(\"Choose the relevant skills, enter Stop when done\")\r\n",
        "\r\n",
        "x=1\r\n",
        "# numpy.append(array, value, axis)\r\n",
        "while(x == 1):\r\n",
        "    n = input()\r\n",
        "    if n =='Stop' or n=='stop':\r\n",
        "      x=0\r\n",
        "      break\r\n",
        "    else:\r\n",
        "      arr.append(skills_dict[int(n)])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 'Python', 2: 'Range', 3: 'Jetty', 4: 'Io', 5: 'Cp', 6: 'Hadoop', 7: 'Load', 8: 'Pipes', 9: 'Pattern', 10: 'Testing', 11: 'Structured data', 12: 'Computer security', 13: 'Ansible', 14: 'Design', 15: 'Ruby', 16: 'Devops', 17: 'Singleton', 18: 'Partition', 19: 'C#', 20: 'Los angeles', 21: 'Web2py', 22: 'Computer science', 23: 'Operations', 24: 'Elasticsearch', 25: 'Mumbai', 26: 'Row', 27: 'Asp.net web api', 28: 'Jenkins', 29: 'Availability', 30: 'Kafka', 31: 'Unix', 32: 'Nunit', 33: 'Replication', 34: 'Ubuntu', 35: 'Rdbms', 36: 'Asp.net', 37: 'Technology', 38: 'Mysql', 39: 'Binary', 40: 'Sli', 41: 'Java', 42: 'Principles', 43: 'Mvc', 44: 'Security', 45: 'Logstash', 46: 'Bash', 47: 'Web', 48: 'Application', 49: 'Languages', 50: 'Stored procedures', 51: 'Nosql', 52: 'Search', 53: 'C', 54: 'Sql', 55: 'Framework', 56: 'Database', 57: 'Led', 58: 'Wpf', 59: 'Vulnerability', 60: 'Capital', 61: 'Merge', 62: 'App', 63: 'Strategy', 64: 'Redis', 65: 'Optimization', 66: 'Aws', 67: 'Docker', 68: 'Algorithms', 69: 'Payments', 70: 'C++', 71: 'Git', 72: 'Sorting', 73: 'It', 74: 'Metrics', 75: 'Agile', 76: 'Api', 77: 'Kernel', 78: 'Flask', 79: 'Processing', 80: 'Vm', 81: 'Streams', 82: 'Etl', 83: 'Operating systems', 84: 'Edge', 85: 'Nginx', 86: 'Writing', 87: 'Connect', 88: 'Pay', 89: 'Mar', 90: 'Indexing', 91: 'Pipeline', 92: 'Analytics', 93: 'Dropwizard', 94: 'Amazon', 95: 'Subscribe', 96: 'Repository', 97: 'Storage', 98: 'Store', 99: 'Avl'}\n",
            "Choose the relevant skills, enter Stop when done\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "stop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs0PB9nsdix6",
        "outputId": "31f1a415-bbd9-40ef-f07a-a5abc482a143"
      },
      "source": [
        "print(arr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Python', 'Range', 'Jetty', 'Io', 'Cp']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_kfprKQd7AD",
        "outputId": "b4ce036a-6be7-4bce-f62f-2a07c4e031bf"
      },
      "source": [
        "new_skills={}\r\n",
        "\r\n",
        "for i in range(len(arr)):\r\n",
        "  new_skills[i+1]=arr[i]\r\n",
        "\r\n",
        "print(new_skills)\r\n",
        "i=len(arr)\r\n",
        "g=i+1\r\n",
        "val3 = input(print(\"do you want to add any other skills? (Y/N) \\n\"))\r\n",
        "if(val3 == 'Y' or val3 == 'y'):\r\n",
        "  x=1\r\n",
        "  while(x == 1):\r\n",
        "    print('Enter skill ')\r\n",
        "    n = input()\r\n",
        "    if n =='Stop' or n=='stop':\r\n",
        "      x=0\r\n",
        "      break\r\n",
        "    else:      \r\n",
        "      new_skills[g]=n\r\n",
        "      g+=1\r\n",
        "      \r\n",
        "\r\n",
        "# i=1\r\n",
        "# for value in skills_dict:\r\n",
        "#   for j in range(len(arr)):\r\n",
        "#     print(skills_dict[key])    \r\n",
        "#     if skills_dict[key] == arr[j]:\r\n",
        "#       print(skills_dict.get(key))\r\n",
        "#       new_skills[i]=skills_dict.get(key)\r\n",
        "print(new_skills)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 'Python', 2: 'Range', 3: 'Jetty', 4: 'Io', 5: 'Cp'}\n",
            "do you want to add any other skills? (Y/N) \n",
            "\n",
            "N\n",
            "{1: 'Python', 2: 'Range', 3: 'Jetty', 4: 'Io', 5: 'Cp'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2FH-wpQyga8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "423fdf6a-2c53-474a-f7b0-1a89766129de"
      },
      "source": [
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load pre-trained model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Grad all general stop words\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "# Education Degrees\n",
        "EDUCATION = [\n",
        "            'BE','B.E.', 'B.E', 'BS', 'B.S', 'NIT', \n",
        "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
        "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
        "            'SSC', 'HSC', 'CBSE', 'ICSE', 'XII', 'X', 'PHD',\n",
        "            'Bachelor of Technology', 'Master of Computer Science'\n",
        "        ]\n",
        "\n",
        "def extract_education(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "\n",
        "    # Sentence Tokenizer\n",
        "    nlp_text = [sent.string.strip() for sent in nlp_text.sents]\n",
        "    edu = {}\n",
        "    # Extract education degree\n",
        "    for index, text in enumerate(nlp_text):\n",
        "        for tex in text.split():\n",
        "            # Replace all special symbols\n",
        "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
        "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
        "                try:\n",
        "                    if index + 1 < len(nlp_text):\n",
        "                        if tex in edu:\n",
        "                            edu[tex] += text + nlp_text[index + 1]\n",
        "                        else:\n",
        "                            edu[tex] = text + nlp_text[index + 1]\n",
        "                    if index - 1 < len(nlp_text):\n",
        "                        if tex in edu:\n",
        "                            edu[tex] += nlp_text[index - 1]\n",
        "                        else:\n",
        "                            edu[tex] = nlp_text[index - 1]\n",
        "                    if index + 2 < len(nlp_text):\n",
        "                        if tex in edu:\n",
        "                            edu[tex] += nlp_text[index + 2]\n",
        "                        else:\n",
        "                            edu[tex] = nlp_text[index + 2]\n",
        "                    if index - 2 < len(nlp_text):\n",
        "                        if tex in edu:\n",
        "                            edu[tex] += nlp_text[index - 2] \n",
        "                        else:\n",
        "                            edu[tex] = nlp_text[index - 2]\n",
        "                    if index + 3 < len(nlp_text):\n",
        "                        if tex in edu:\n",
        "                            edu[tex] += nlp_text[index + 3] \n",
        "                        else:\n",
        "                            edu[tex] = nlp_text[index + 3]\n",
        "                    if index - 3 < len(nlp_text):\n",
        "                        if tex in edu:\n",
        "                            edu[tex] += nlp_text[index - 3]\n",
        "                        else:\n",
        "                            edu[tex] = nlp_text[index - 3]\n",
        "                    if index - 4 < len(nlp_text):\n",
        "                        if tex in edu:\n",
        "                            edu[tex] += nlp_text[index - 4]\n",
        "                        else:\n",
        "                            edu[tex] = nlp_text[index - 4]\n",
        "                except:\n",
        "                    edu[tex] = text\n",
        "    print(edu)\n",
        "    # Extract year\n",
        "    education = []\n",
        "    for key in edu.keys():\n",
        "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
        "        if year:\n",
        "            education.append((key, ''.join(year[0])))\n",
        "        else:\n",
        "            education.append(key)\n",
        "    return education"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI5_Dvs6PNhL",
        "outputId": "c65fd955-fbf5-4bb7-f239-a87f1adf4e48"
      },
      "source": [
        "education = extract_education(text)\r\n",
        "print(education)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{}\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWnuAy8jPPZ_",
        "outputId": "7681e5a0-8217-4ffe-9dc2-a6c230e82ccd"
      },
      "source": [
        "result = {'education': education, 'email': email, 'mobile_number': mobile_number, 'skills': new_skills}\r\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'education': [], 'email': 'sandhya2392@gmail.com|', 'mobile_number': '9495220190', 'skills': {1: 'Python', 2: 'Range', 3: 'Jetty', 4: 'Io', 5: 'Cp'}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4AXHNwZPPTS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}