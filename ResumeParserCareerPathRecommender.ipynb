{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResumeParserCareerPathRecommender.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMwacZKnpuYTACsCguHBAml",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saigirishgilly98/Major_Project/blob/main/ResumeParserCareerPathRecommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNrTluJqzyEx"
      },
      "source": [
        "## Reference : https://omkarpathak.in/2018/12/18/writing-your-own-resume-parser/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKCbDq5Ttue8",
        "outputId": "3e8c94cd-fb9d-4ad7-d3ad-06f82e430f83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "! pip install pdfminer.six"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pdfminer.six\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/12/ab5ebafc4cb2b49847de7bfc26f2d152f42a4af136263152d070c61dfd7d/pdfminer.six-20200726-py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 2.7MB/s \n",
            "\u001b[?25hCollecting cryptography\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/62/30f6936941d87a5ed72efb24249437824f6b2c953901245b58c91fde2f27/cryptography-3.1.1-cp35-abi3-manylinux2010_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 44.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.6/dist-packages (from pdfminer.six) (2.2.2)\n",
            "Requirement already satisfied: chardet; python_version > \"3.0\" in /usr/local/lib/python3.6/dist-packages (from pdfminer.six) (3.0.4)\n",
            "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from cryptography->pdfminer.six) (1.15.0)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography->pdfminer.six) (1.14.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography->pdfminer.six) (2.20)\n",
            "Installing collected packages: cryptography, pdfminer.six\n",
            "Successfully installed cryptography-3.1.1 pdfminer.six-20200726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxYYqwFsvy6L"
      },
      "source": [
        "import io"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrVD7uQNu_ct"
      },
      "source": [
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.pdfinterp import PDFPageInterpreter\n",
        "from pdfminer.pdfinterp import PDFResourceManager\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as fh:\n",
        "        # iterate over all pages of PDF document\n",
        "        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
        "            # creating a resoure manager\n",
        "            resource_manager = PDFResourceManager()\n",
        "            \n",
        "            # create a file handle\n",
        "            fake_file_handle = io.StringIO()\n",
        "            \n",
        "            # creating a text converter object\n",
        "            converter = TextConverter(\n",
        "                                resource_manager, \n",
        "                                fake_file_handle, \n",
        "                                codec='utf-8', \n",
        "                                laparams=LAParams()\n",
        "                        )\n",
        "\n",
        "            # creating a page interpreter\n",
        "            page_interpreter = PDFPageInterpreter(\n",
        "                                resource_manager, \n",
        "                                converter\n",
        "                            )\n",
        "\n",
        "            # process current page\n",
        "            page_interpreter.process_page(page)\n",
        "            \n",
        "            # extract text\n",
        "            text = fake_file_handle.getvalue()\n",
        "            yield text\n",
        "\n",
        "            # close open handles\n",
        "            converter.close()\n",
        "            fake_file_handle.close()\n",
        "\n",
        "# calling above function and extracting text\n",
        "text = ''\n",
        "for page in extract_text_from_pdf('Abhishak_Resume.pdf'):\n",
        "    text += ' ' + page\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9JAPWpjvQGo",
        "outputId": "faad7ddf-eb30-47fe-cfd3-012bdab66302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "text"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' R abhishakvarshney@gmail.com\\n\\nﬂ linkedin.com/in/abhishakvarshney\\n\\n(cid:135) github.com/abhishakvarshney\\n\\nABHISHAK VARSHNEY\\n\\n(cid:211) +91-8433489919\\n(cid:7) live:abhishakvarshney\\n\\nEDUCATION\\n\\nNIT Jaipur(MNIT/MREC)\\nB.Tech - Metallurgical & Materials Engineering. CGPA: 7.5\\n\\n(cid:17) 2014 – 2018\\nB.B.S.S.M. Inter College\\nIntermediate/+2; Uttar-Pradesh Board Result: 91.00%\\n\\n(cid:17) 2012 – 2013\\nB.B.S.S.M. Inter College\\nHigh-School; Uttar-Pradesh Board Result: 72.83%\\n\\n(cid:17) 2010 – 2011\\n\\nEXPERIENCE\\n\\nSoftware Engineer - Analytics\\nSkilrock Technologies | Sugal & Damani Group\\n\\n(cid:17) June 2018 – Present\\n\\n‰ Gurugram,India\\n\\nNLP - ChatBot (ARENA)\\n• Developed an NLP based chatbot in Python from that users can\\nplay games, purchase tickets & can chat small talk with user.\\n• Deployed on Skilrock Technologies Client Gaming and Lottery\\nEngine website, Android and iOS Apps and on Facebook Mes-\\nsenger.\\n\\n• Technology: Python | Rasa, Microsoft Bot Framework\\n\\nTrainee | INTERNSHIP\\nTata Steel\\n\\n(cid:17) May 2017 - July 2017\\n\\n‰ Jamshedpur,India\\n\\nHeat & Mass Balance in BOF Vessel\\n• Analyzed and Balanced Heat and Mass data from Raw Material\\n\\nto liquid steel making process i.e. from raw to production.\\n\\n• Technology: MS – Excel\\n\\nCERTIFICATION\\n\\n• Machine Learning: Coursera\\n• R Basics – R Programming Language Introduction: Udemy\\n• Introduction to Python Programming: Udemy\\n• MongoDB Basics: MongoDB Inc. – MongoDB University\\n• SQL Fundamental Course: SOLOLearn\\nACHIEVEMENTS\\n\\n• Placed in top 21% in Housing Price Prediction Kaggle challenge\\n• Secured II rank in ’International Robotics Challenge – Sixth\\n\\nSense Robotics – 2014’ organized by ’ROBOTech Labs and IIT-\\nBombay’.\\n\\n• Participated in ‘58th National School Skating Championship\\n\\n2012-13’ organized by ‘Indian Olympic Association’\\n\\n• Secured I rank in ‘State Skating Championship-2012’ organized\\n\\nby ‘Vidya-Bharti’ at Meerut.\\n\\nSKILLS\\n\\n• Development & Machine Learning: Python\\n• Analytics & Visualisation Tools: R - Language,\\n\\nMS-Excel, Google Analytics\\n\\n• Database Languages: MySQL, MongoDB\\n• Tools & Frameworks: Git, Nginx, Supervisord\\n\\nTECHNICAL PROFILES\\n\\n• Rasa Community | Chatbot :\\n\\n@abhishakskilrock – Rank: 15/2992\\n\\n• Kaggle | Data Science :\\n\\n@abhishakvarshney – Rank: Top 25%\\n\\n• HackerRank | Programming :\\n\\n@abhishakvarshney\\n\\nPROJECTS\\n\\nStanford Open Policing Project- California\\n• Prediction of trafﬁc stop rates, their times’ and\\n\\nplaces that reliably generate stops.\\n\\n• Technology: Time-Series Analysis | ARIMA\\n\\nModel | R Language\\n\\nHousing Price Prediction\\n• Prediction of Sale Price of Houses in USA\\n\\nbased on various features.\\n\\n• Technology: Random Forest | R Language\\n\\nTwitter Text Mining\\n• Extract data from twitter and Predict the senti-\\nments of four pharma companies: Bayer, Pﬁzer,\\nRoche and Novartis.\\n\\n• Technology: Naive Bayes Theorem | R Lan-\\n\\nguage\\n\\nImage Processing: Object Detection\\n• A Python based application which can detect\\ndifferent objects. Detected racoon, horses,\\ndogs and cat in various images and videos using\\ntrained data/images.\\n\\n• Technology: CNN, YOLO | Python - Tensor-\\n\\nﬂow, Keras, OpenCV\\n\\nLinux Path Traversal\\n• Created virtual linux terminal that can execute\\nvarious commands: md, cd, cd.., ls, pwd, dir, rm,\\ncp, mv, session_clean.\\n• Technology: Python\\n\\nHOBBIES\\n\\nSkating | Travelling\\n\\n\\x0c'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeiR0jXnv-SX",
        "outputId": "8e8ecc18-718b-489c-a10a-44adbde7ad94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "source": [
        "! pip install spacy\n",
        "! python -m spacy download en_core_web_sm"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.2.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.2.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obr-eaEzw9sB"
      },
      "source": [
        "## Extracting Name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvSV4W44wRll"
      },
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# load pre-trained model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# initialize matcher with a vocab\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "def extract_name(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "    \n",
        "    # First name and Last name are always Proper Nouns\n",
        "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
        "    \n",
        "    matcher.add('NAME', None, pattern)\n",
        "    \n",
        "    matches = matcher(nlp_text)\n",
        "    \n",
        "    for match_id, start, end in matches:\n",
        "        span = nlp_text[start:end]\n",
        "        print(span.text)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_0punAUwkZ0"
      },
      "source": [
        "extract_name(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGsI_x-oxCTk"
      },
      "source": [
        "## Extracting Phone Numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1eFcv7lwoI0"
      },
      "source": [
        "import re\n",
        "\n",
        "def extract_mobile_number(text):\n",
        "    flag = 0\n",
        "    phone = re.findall(re.compile(r'(\\d{3}-\\d{3}-\\d{4})'), text)\n",
        "    if len(phone) == 0:\n",
        "        phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\n",
        "        flag = 1\n",
        "    print(phone)\n",
        "    if phone:\n",
        "        if flag == 1:\n",
        "            number = ''.join(phone[0])\n",
        "        else:\n",
        "            number = ''.join(phone[0].split('-'))\n",
        "        if len(number) > 10:\n",
        "            return '+' + number\n",
        "        else:\n",
        "            return number"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W66eX0sQxK_u",
        "outputId": "928edc70-bbce-4060-889b-fcffc4d95dab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "mobile_number = extract_mobile_number(text)\n",
        "print(mobile_number)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('91', '', '84', '334', '8991', '')]\n",
            "+91843348991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9W5MXXDxUsG"
      },
      "source": [
        "## Extracting E-Mail"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3w3WQi_xOyo"
      },
      "source": [
        "import re\n",
        "\n",
        "def extract_email(email):\n",
        "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", email)\n",
        "    if email:\n",
        "        try:\n",
        "            return email[0].split()[0].strip(';')\n",
        "        except IndexError:\n",
        "            return None"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZNh1oR5xZ77",
        "outputId": "d4db1316-f4a6-442f-98bc-bf41c37567bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "email = extract_email(text)\n",
        "print(email)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "abhishakvarshney@gmail.com\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kexrZN-Ix7SY"
      },
      "source": [
        "## Extracting Skills"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uwz1kpfLxdD5"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRxAaScsx-Hk",
        "outputId": "5f274ca2-b02f-4c7a-bbbd-4149801b1af6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "skills_dataset = pd.read_csv('linkedin_skill.txt', sep='\\n', header=None)\n",
        "skills_dataset.columns = ['skill']\n",
        "skills_dataset.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>skill</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(ISC)2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>.NET</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>.NET CLR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>.NET Compact Framework</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>.NET Framework</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    skill\n",
              "0                  (ISC)2\n",
              "1                    .NET\n",
              "2                .NET CLR\n",
              "3  .NET Compact Framework\n",
              "4          .NET Framework"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XficZdAnyEPQ",
        "outputId": "97cc563b-62a5-4090-84e4-1cd1594f581c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "skills = []\n",
        "for i in range(len(skills_dataset)):\n",
        "    skills.append(str(skills_dataset.loc[i, 'skill']).lower())\n",
        "skills[-20:]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['zoominfo',\n",
              " 'zoomtext',\n",
              " 'zoomerang',\n",
              " 'zope',\n",
              " 'zotero',\n",
              " 'zsh',\n",
              " 'zuken',\n",
              " 'zultys',\n",
              " 'zulu education products',\n",
              " 'zuludesk',\n",
              " 'zumba',\n",
              " 'zumba instruction',\n",
              " 'zuora',\n",
              " 'zymography',\n",
              " 'zynx',\n",
              " 'zyxel',\n",
              " 'z/os',\n",
              " 'z/vm',\n",
              " 'zlinux',\n",
              " 'zseries']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OeajW-iyWOT"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# load pre-trained model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def extract_skills(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "\n",
        "    # removing stop words and implementing word tokenization\n",
        "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
        "    \n",
        "    skillset = []\n",
        "    \n",
        "    # check for one-grams (example: python)\n",
        "    for token in tokens:\n",
        "        if token.lower() in skills:\n",
        "            skillset.append(token)\n",
        "    \n",
        "    # check for bi-grams and tri-grams (example: machine learning)\n",
        "    for token in nlp_text.noun_chunks:\n",
        "        token = token.text.lower().strip()\n",
        "        if token in skills:\n",
        "            skillset.append(token)\n",
        "    \n",
        "    return [i.capitalize() for i in set([i.lower() for i in skillset])]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E70Y4K4ydBc",
        "outputId": "d1ce4a46-2903-4b1e-f22b-2324574684a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "skills = extract_skills(text)\n",
        "print(skills)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Learning', 'Mass balance', 'Coursera', 'Mysql', 'Arima', 'Hobbies', 'Opencv', 'Linux', 'Google', 'Twitter', 'Skating', 'Terminal', 'Machine learning', 'Keras', 'Analytics', 'Forest', 'Nlp', 'Languages', 'Framework', 'Excel', 'Mongodb inc.', 'Robotics', 'Microsoft', 'Git', 'Horses', 'Steel', 'Mining', 'Features', 'Live', 'Prediction', 'Materials', 'Application', 'R', 'Facebook', 'Apps', 'Processing', 'Visualisation', 'Database', 'Android', 'Dogs', 'Balance', 'Small talk', 'Sql', 'Text mining', 'Profiles', 'Cd', 'Object detection', 'Gaming', 'Bot', 'Ios', 'Mv', 'Olympic', 'Cp', 'Games', 'Python', 'Image processing', 'Google analytics', 'Technology', 'Mongodb', 'Chat', 'Nginx', 'Heat']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8oXPpR2yjnW"
      },
      "source": [
        "## Extracting Education"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2FH-wpQyga8",
        "outputId": "205b653d-8f9b-4133-d600-4a85ee1d934a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load pre-trained model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Grad all general stop words\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "# Education Degrees\n",
        "EDUCATION = [\n",
        "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
        "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
        "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
        "            'SSC', 'HSC', 'CBSE', 'ICSE', 'XII', 'X', 'PHD'\n",
        "        ]\n",
        "\n",
        "def extract_education(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "\n",
        "    # Sentence Tokenizer\n",
        "    nlp_text = [sent.string.strip() for sent in nlp_text.sents]\n",
        "    edu = {}\n",
        "    # Extract education degree\n",
        "    for index, text in enumerate(nlp_text):\n",
        "        for tex in text.split():\n",
        "            # Replace all special symbols\n",
        "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
        "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
        "                try:\n",
        "                    if index + 1 < len(nlp_text):\n",
        "                        if tex in edu:\n",
        "                            edu[tex] += text + nlp_text[index + 1]\n",
        "                        else:\n",
        "                            edu[tex] = text + nlp_text[index + 1]\n",
        "                    if index - 1 < len(nlp_text):\n",
        "                        if tex in edu:\n",
        "                            edu[tex] += nlp_text[index - 1]\n",
        "                        else:\n",
        "                            edu[tex] = nlp_text[index - 1]\n",
        "                    if index + 2 < len(nlp_text):\n",
        "                        if tex in edu:\n",
        "                            edu[tex] += nlp_text[index + 2]\n",
        "                        else:\n",
        "                            edu[tex] = nlp_text[index + 2]\n",
        "                    if index - 2 < len(nlp_text):\n",
        "                        if tex in edu:\n",
        "                            edu[tex] += nlp_text[index - 2] \n",
        "                        else:\n",
        "                            edu[tex] = nlp_text[index - 2]\n",
        "                    if index + 3 < len(nlp_text):\n",
        "                        if tex in edu:\n",
        "                            edu[tex] += nlp_text[index + 3] \n",
        "                        else:\n",
        "                            edu[tex] = nlp_text[index + 3]\n",
        "                    if index - 3 < len(nlp_text):\n",
        "                        if tex in edu:\n",
        "                            edu[tex] += nlp_text[index - 3]\n",
        "                        else:\n",
        "                            edu[tex] = nlp_text[index - 3]\n",
        "                    if index - 4 < len(nlp_text):\n",
        "                        if tex in edu:\n",
        "                            edu[tex] += nlp_text[index - 4]\n",
        "                        else:\n",
        "                            edu[tex] = nlp_text[index - 4]\n",
        "                except:\n",
        "                    edu[tex] = text\n",
        "    print(edu)\n",
        "    # Extract year\n",
        "    education = []\n",
        "    for key in edu.keys():\n",
        "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
        "        if year:\n",
        "            education.append((key, ''.join(year[0])))\n",
        "        else:\n",
        "            education.append(key)\n",
        "    return education"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpZkCcbxyo7j",
        "outputId": "c51d8bbe-353a-4b4c-beba-bebd16af231b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "education = extract_education(text)\n",
        "print(education)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'BTech': 'B.Tech - Metallurgical & Materials Engineering.CGPA:NIT Jaipur(MNIT/MREC)7.5EDUCATION(cid:17) 2014 – 2018abhishakvarshney(cid:7) live:', 'MS': '• Technology: MS –Excel• Analyzed and Balanced Heat and Mass data from Raw Material\\n\\nto liquid steel making process i.e. from raw to production.CERTIFICATIONJamshedpur,India\\n\\nHeat & Mass Balance in BOF Vessel• Machine Learning: Coursera- July 2017\\n\\n‰May 2017'}\n",
            "[('BTech', '2014'), ('MS', '2017')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfP6NklizloN",
        "outputId": "8d9aecf3-2436-4ff6-b26a-fd8ecc61f5cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "result = {'education': education, 'email': email, 'mobile_number': mobile_number, 'skills': skills}\n",
        "print(result)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'education': [('BTech', '2014'), ('MS', '2017')], 'email': 'abhishakvarshney@gmail.com', 'mobile_number': '+91843348991', 'skills': ['Learning', 'Mass balance', 'Coursera', 'Mysql', 'Arima', 'Hobbies', 'Opencv', 'Linux', 'Google', 'Twitter', 'Skating', 'Terminal', 'Machine learning', 'Keras', 'Analytics', 'Forest', 'Nlp', 'Languages', 'Framework', 'Excel', 'Mongodb inc.', 'Robotics', 'Microsoft', 'Git', 'Horses', 'Steel', 'Mining', 'Features', 'Live', 'Prediction', 'Materials', 'Application', 'R', 'Facebook', 'Apps', 'Processing', 'Visualisation', 'Database', 'Android', 'Dogs', 'Balance', 'Small talk', 'Sql', 'Text mining', 'Profiles', 'Cd', 'Object detection', 'Gaming', 'Bot', 'Ios', 'Mv', 'Olympic', 'Cp', 'Games', 'Python', 'Image processing', 'Google analytics', 'Technology', 'Mongodb', 'Chat', 'Nginx', 'Heat']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDtzVoHzzqa0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}